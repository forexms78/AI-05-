{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPyveX6RN1UeByoxnAVGPuF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/forexms78/AI-05-/blob/main/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yZt5bWGRqslt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#def scaled_dot_product_attention\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "\n",
        "  matmul_qk = torch.matmul(q, k.transpose(-1, -2)) #곱하기 쿼리하고 키\n",
        "\n",
        "  dk = q.size(-1)\n",
        "  scaled_attention_logits = matmul_qk / math.sqrt(dk) #스케일링\n",
        "\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits = scaled_attention_logits.masked_fill(mask == False, float('-1e9'))\n",
        "\n",
        "  #softmax\n",
        "  attention_weights = F.softmax(scaled_attention_logits, dim=-1)\n",
        "\n",
        "  output = torch.matmul(attention_weights, v)\n",
        "\n",
        "  return output, attention_weights\n",
        "\n",
        "#test code\n",
        "x = torch.randn(3,20,64)\n",
        "out, atw = scaled_dot_product_attention(x,x,x, mask=None)\n",
        "\n",
        "print(f'attention output shape : {out.shape}')\n",
        "print(f'attention weight shape : {atw.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kP_aNhuVq9Pc",
        "outputId": "d202d451-564f-4748-9b97-8fc6754d2384"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention output shape : torch.Size([3, 20, 64])\n",
            "attention weight shape : torch.Size([3, 20, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title multihead attention\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, em_dim, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "    self.em_dim = em_dim # 임베딩 차원 설정\n",
        "    self.num_heads = num_heads # 어텐션 헤드 개수 설정\n",
        "\n",
        "    self.head_dim = em_dim // num_heads # 각 어텐션 헤드의 차원 계산\n",
        "\n",
        "    self.wq = nn.Linear(em_dim, em_dim) # 쿼리 변환을 위한 선형 레이어 정의\n",
        "    self.wk = nn.Linear(em_dim, em_dim) # 키 변환을 위한 선형 레이어 정의\n",
        "    self.wv = nn.Linear(em_dim, em_dim) # 값 변환을 위한 선형 레이어 정의\n",
        "\n",
        "    self.dense = nn.Linear(em_dim, em_dim) # 최종 출력을 위한 선형 레이어 정의\n",
        "\n",
        "\n",
        "  #split_heads\n",
        "  def split_heads(self, x):\n",
        "    batch_size, seq_len, em_dim = x.size() # 입력 텐서의 배치 크기, 시퀀스 길이, 임베딩 차원 가져오기\n",
        "    x = x.view(batch_size, seq_len, self.num_heads, self.head_dim) # 어텐션 헤드 개수와 헤드 차원에 맞춰 텐서 모양 변경\n",
        "    return x.permute(0, 2, 1, 3) # 어텐션 계산을 위해 차원 순서 변경 (batch_size, num_heads, seq_len, head_dim)\n",
        "\n",
        "  #forward\n",
        "  def forward(self, q, k, v, mask=None):\n",
        "    batch_size = q.size(0) # 배치 크기 가져오기\n",
        "\n",
        "    q = self.wq(q) # 쿼리 선형 변환 적용\n",
        "    k = self.wk(k) # 키 선형 변환 적용\n",
        "    v = self.wv(v) # 값 선형 변환 적용\n",
        "\n",
        "    q = self.split_heads(q) # 쿼리를 여러 헤드로 분할\n",
        "    k = self.split_heads(k) # 키를 여러 헤드로 분할\n",
        "    v = self.split_heads(v) # 값을 여러 헤드로 분할\n",
        "\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask) # 스케일드 닷 프로덕트 어텐션 계산\n",
        "\n",
        "    scaled_attention = scaled_attention.permute(0,2,1,3).contiguous() # 어텐션 결과를 원래 차원 순서로 되돌리기\n",
        "    concat_attention = scaled_attention.view(batch_size, -1, self.em_dim) # 여러 헤드의 어텐션 결과를 하나로 연결\n",
        "\n",
        "    output = self.dense(concat_attention) # 최종 선형 변환 적용\n",
        "\n",
        "    return output, attention_weights # 어텐션 결과와 어텐션 가중치 반환\n",
        "\n",
        "# test\n",
        "x = torch.randn(2,10,64)\n",
        "mh = MultiHeadAttention(em_dim=64, num_heads=2)\n",
        "out, atw = mh(x,x,x)\n",
        "print(f'attention output shape : {out.shape}')\n",
        "print(f'attention weight shape : {atw.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ6kL7FyuSIE",
        "outputId": "dd7530bf-9b2e-47f2-85e3-4fb737b76c57"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention output shape : torch.Size([2, 10, 64])\n",
            "attention weight shape : torch.Size([2, 2, 10, 10])\n"
          ]
        }
      ]
    }
  ]
}