{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOJITzdT4rWU0EDl2HS4Jws",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/forexms78/AI-05-/blob/main/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# íŠ¸ëœìŠ¤í¬ë¨¸\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "rfCaBMnpcqAz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "acW2D3ifHfgr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer ëª¨ë¸ êµ¬ì¡°\n",
        "\n",
        "TransformerëŠ” Seq2seqì™€ ë¹„ìŠ·í•˜ê²Œ ê¸°ê³„ë²ˆì—­ì„ í•´ê²°í•˜ê¸° ìœ„í•œ ì¸ì½”ë”ì™€ ë””ì½”ë”êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. Seq2seqì™€ëŠ” ë‹¬ë¦¬ ì¸ì½”ë” ì™€ ë””ì½”ë” ë‚´ë¶€ì—ëŠ” MultiHeadAttention ë¸”ë¡ê³¼ FeedForwaedë¼ëŠ” ë¸”ë¡ìœ¼ë¡œ êµ¬ì„±\n",
        "\n",
        "<center><img src=\"https://arxiv.org/html/1706.03762v7/extracted/1706.03762v7/Figures/ModalNet-21.png\" width=\"300\"/></center>\n",
        "\n",
        "ì¶œì²˜: `https://arxiv.org/abs/1706.03762`"
      ],
      "metadata": {
        "id": "GojRdH_Vdxmu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xnPJwvMq-xFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# ğŸ§© Transformer êµ¬í˜„ ë‹¨ê³„ë³„ ê°€ì´ë“œ\n",
        "\n",
        "---\n",
        "\n",
        "## âš™ï¸ 1ë‹¨ê³„: *Scaled Dot-Product Attention*\n",
        "íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì‹¬ì¥ì¸ **Attention(Q, K, V)** ê³µì‹ì„ ì½”ë“œë¡œ ì§ì ‘ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
        "\n",
        "$$\n",
        "Attention(Q, K, V) = softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "$$\n",
        "\n",
        "\n",
        "- Query(Q), Key(K), Value(V) ë²¡í„° ê°„ ìœ ì‚¬ë„ ê³„ì‚°  \n",
        "- Softmaxë¡œ ì¤‘ìš”ë„ë¥¼ êµ¬í•˜ê³ , Valueì— ê°€ì¤‘í•©  \n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ§  2ë‹¨ê³„: *Multi-Head Attention*\n",
        "1ë‹¨ê³„ì—ì„œ ë§Œë“  ì–´í…ì…˜ì„ ì—¬ëŸ¬ ê°œ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ì—¬ â€œë‹¤ì–‘í•œ ê´€ì â€ì—ì„œ ë¬¸ì¥ì„ ë°”ë¼ë³¼ ìˆ˜ ìˆë„ë¡ í™•ì¥í•©ë‹ˆë‹¤.\n",
        "\n",
        "- ì—¬ëŸ¬ í—¤ë“œ(`num_heads`)ë¥¼ í†µí•´ ë³‘ë ¬ ì–´í…ì…˜ ìˆ˜í–‰  \n",
        "- ê° í—¤ë“œì˜ ì¶œë ¥ì„ ê²°í•©(concat) í›„ ì„ í˜• ë³€í™˜  \n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ›¡ï¸ 3ë‹¨ê³„: *Padding & Look-Ahead Masks*\n",
        "ì–´í…ì…˜ì´ ë¶ˆí•„ìš”í•œ `<pad>` í† í°ì„ ë¬´ì‹œí•˜ê³ , ë””ì½”ë”ê°€ ë¯¸ë˜ ë‹¨ì–´ë¥¼ **ë¯¸ë¦¬ ì—¿ë³´ì§€ ëª»í•˜ê²Œ** í•©ë‹ˆë‹¤.\n",
        "\n",
        "- **Padding Mask** â†’ `<pad>` ìœ„ì¹˜ë¥¼ ê°€ë ¤ ê³„ì‚° ì œì™¸  \n",
        "- **Look-Ahead Mask** â†’ ë””ì½”ë”ê°€ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ë³´ì§€ ëª»í•˜ë„ë¡ ì œí•œ  \n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ—ï¸ 4ë‹¨ê³„: ì¸ì½”ë” ìœ ë‹› ì¡°ë¦½ â€” *EncoderLayer*\n",
        "**MultiHeadAttention + FeedForwardNetwork + Add & Norm**\n",
        "\n",
        "- Self-Attention â†’ FFN  \n",
        "- Residual Connection + LayerNorm  \n",
        "- ì¸ì½”ë”ì˜ ê¸°ë³¸ ë ˆì´ì–´ êµ¬ì¡° ì™„ì„±  \n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ§© 5ë‹¨ê³„: ë””ì½”ë” ìœ ë‹› ì¡°ë¦½ â€” *DecoderLayer*\n",
        "ì¸ì½”ë”ë³´ë‹¤ ë³µì¡í•œ êµ¬ì¡°ë¥¼ ê°€ì§‘ë‹ˆë‹¤.\n",
        "\n",
        "1. Masked Multi-Head Attention (ìê¸° ì–´í…ì…˜)  \n",
        "2. Encoderâ€“Decoder Attention (ì¸ì½”ë” ì¶œë ¥ê³¼ì˜ ì–´í…ì…˜)  \n",
        "3. Feed Forward Network  \n",
        "\n",
        "ê° ë¶€ë¶„ë§ˆë‹¤ Dropout, Residual, LayerNormì´ ì ìš©ë©ë‹ˆë‹¤.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ§± 6ë‹¨ê³„: ìµœì¢… ëª¨ë¸ ì¡°ë¦½ â€” *Encoder, Decoder, Transformer*\n",
        "4, 5ë‹¨ê³„ì—ì„œ ë§Œë“  ì¸µì„ Nê°œì”© ìŒ“ê³ , **Positional Encoding**ì„ ì¶”ê°€í•´ ìˆœì„œ ì •ë³´ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.\n",
        "\n",
        "- Encoder: ì…ë ¥ ë¬¸ì¥ ì¸ì½”ë”©  \n",
        "- Decoder: íƒ€ê¹ƒ ë¬¸ì¥ ìƒì„±  \n",
        "- Transformer: Encoder + Decoder ê²°í•© ëª¨ë¸ ì™„ì„±  \n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ—‚ï¸ 7ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„ â€” *Tokenizer & Dataset*\n",
        "í•œêµ­ì–´ ì±—ë´‡ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™€ SentencePiece í† í¬ë‚˜ì´ì € í•™ìŠµ\n",
        "\n",
        "- SentencePieceë¡œ ë‹¨ì–´ ë¶„ì ˆ  \n",
        "- í…ìŠ¤íŠ¸ë¥¼ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜  \n",
        "- Padding ì ìš© â†’ `PyTorch Dataset` êµ¬ì„±  \n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ§® 8ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ â€” *Training Loop*\n",
        "Transformer í•™ìŠµì„ ìœ„í•œ í•™ìŠµ ë£¨í”„ êµ¬ì„±\n",
        "\n",
        "- **ëª¨ë¸**: Transformer  \n",
        "- **Optimizer**: Adam  \n",
        "- **Loss Function**: CrossEntropyLoss(ignore_index=pad_token)  \n",
        "- **í›ˆë ¨ ë°˜ë³µ**: forward â†’ loss â†’ backward â†’ optimizer.step()\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ§ª 9ë‹¨ê³„: ì¶”ë¡  ë° í‰ê°€ â€” *Generation & BLEU*\n",
        "í•™ìŠµëœ ëª¨ë¸ì´ ë¬¸ì¥ì„ **ìŠ¤ìŠ¤ë¡œ ìƒì„±(Generate)** í•˜ë„ë¡ í•œ í›„, ì‹¤ì œ ì •ë‹µê³¼ ë¹„êµí•˜ì—¬ BLEU ì ìˆ˜ë¡œ í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤.\n",
        "\n",
        "- `generate()` í•¨ìˆ˜ë¡œ ë²ˆì—­ ë¬¸ì¥ ìƒì„±  \n",
        "- **BLEU Score**: ìƒì„±ëœ ë¬¸ì¥ê³¼ ì •ë‹µ ë¬¸ì¥ì˜ n-gram ìœ ì‚¬ë„ í‰ê°€  \n"
      ],
      "metadata": {
        "id": "2AU9WlLo8s8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scaled Dot-Product Attention\n",
        "\n",
        "ë¨¼ì € MultiHeadAttentionì´ êµ¬ì„±ë˜ê¸° ìœ„í•´ì„œëŠ” ë‚´ë¶€ì ìœ¼ë¡œ Scaled Dot-Product Attention ì—°ì‚°ì´ ì§„í–‰ì´ ë˜ì–´ì•¼ í•¨.  \n",
        "\n",
        "Scaled Dot-Product AttentionëŠ” Queryì™€ Keyë¥¼ **ë‹¨ìˆœ ë‚´ì **í•˜ì—¬ ìŠ¤ì½”ì–´ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
        "\n",
        "<center><img src=\"https://arxiv.org/html/1706.03762v7/extracted/1706.03762v7/Figures/ModalNet-19.png\" width=\"200\"/></center>\n",
        "\n",
        "ì¶œì²˜: `https://arxiv.org/abs/1706.03762`\n",
        "\n",
        "\n",
        "$${Attention}(Q, K, V) = \\text{softmax}\\Bigl(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigr) \\, V$$\n",
        "\n",
        "- Queryì™€ Key ê°„ì˜ ë‹¨ìˆœ ë‚´ì ì„ í™œìš©í•˜ë¯€ë¡œ, ëŒ€ê·œëª¨ ë³‘ë ¬ ì—°ì‚°ì´ ê°€ëŠ¥í•˜ê³  ì‹œí€€ìŠ¤ ì „ì—­ì— ê±¸ì³ ë¹ ë¥´ê²Œ Attention ê³„ì‚°ì„ ìˆ˜í–‰\n",
        "- ì°¨ì›ì´ ì»¤ì§€ëŠ” ë¬¸ì œë¥¼ ìŠ¤ì¼€ì¼ë§(âˆšd_k)ìœ¼ë¡œ ë³´ì •í•¨ìœ¼ë¡œì¨, ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµì´ ê°€ëŠ¥\n",
        "\n",
        "ì´ë•Œ TransformerëŠ” Scaled Dot-Product Attentionì˜  Qurey, Key, Valueë¥¼ ì „ë¶€ ë™ì¼í•œ í…ìŠ¤íŠ¸(ì„ë² ë”©)ì„ ë„£ì–´ ë¬¸ì¥ ë‚´ì˜ ëª¨ë“  ë‹¨ì–´(í† í°) ê°„ ì˜ì¡´ ê´€ê³„ë¥¼ ë™ì‹œì— í•™ìŠµí•˜ëŠ” **Self-Attention** ê¸°ë²•ìœ¼ë¡œ í™œìš©í•©ë‹ˆë‹¤.\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1xpFv-xHu2BnFLkaboNybk5KkQ_vgpwO-\" width=\"600\"/></center>\n"
      ],
      "metadata": {
        "id": "1Y8q_oXjhc1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "    \"\"\"\n",
        "    Dot Product: Queryê°€ ëª¨ë“  Keyì™€ ì–¼ë§ˆë‚˜ â€œìœ ì‚¬í•œì§€(=ì—°ê´€ì„±)â€ë¥¼ ë‚˜íƒ€ëƒ„\n",
        "\n",
        "    ë©€í‹°í—¤ë“œ ì–´í…ì…˜ì—ì„œ ì‚¬ìš©ë  ê²½ìš°\n",
        "    q, k, v : shape = (batch_size, n_heads, seq_len, depth)\n",
        "    mask    : shape = (batch_size, 1, seq_len, seq_len)\n",
        "    \"\"\"\n",
        "    # q, k, v: (batch_size, seq_len, em_dim)\n",
        "    matmul_qk = torch.matmul(q, k.transpose(-1, -2))    # Matrix Multiplication (í–‰ë ¬ ê³±ì…ˆ)\n",
        "\n",
        "    # ìŠ¤ì¼€ì¼ë§: ë‚´ì ê°’ì´ ì°¨ì› ìˆ˜ì— ë”°ë¼ ì»¤ì§€ì§€ ì•Šë„ë¡ âˆšdkë¡œ ë‚˜ëˆ„ì–´ ì•ˆì •í™”í•˜ëŠ” ê³¼ì •\n",
        "    dk = q.size(-1)                                     # Query í…ì„œì˜ ë§ˆì§€ë§‰ ì°¨ì›\n",
        "    scaled_attention_logits = matmul_qk / math.sqrt(dk) # ìŠ¤ì½”ì–´ë¥¼ ì•ˆì •í™”(ì •ê·œí™”)\n",
        "\n",
        "    # ë§ˆìŠ¤í¬ ì ìš©(ë©€í‹°í—¤ë“œ ì–´í…ì…˜)\n",
        "    if mask is not None:\n",
        "        # ë§ˆìŠ¤í¬ê°€ Falseì¸ ë¶€ë¶„ì— ë§¤ìš° í° ìŒìˆ˜(-1e9)ë¥¼ ë”í•´ softmaxì—ì„œ ì œì™¸ë˜ë„ë¡ í•¨\n",
        "        scaled_attention_logits = scaled_attention_logits.masked_fill(mask == False, float('-1e9'))\n",
        "\n",
        "    # ì†Œí”„íŠ¸ë§¥ìŠ¤\n",
        "    attention_weights = F.softmax(scaled_attention_logits, dim=-1)\n",
        "\n",
        "    # Valueì™€ ê³±í•´ì„œ ìµœì¢… ì–´í…ì…˜ ê²°ê³¼\n",
        "    output = torch.matmul(attention_weights, v)       # Matrix Multiplication (í–‰ë ¬ ê³±ì…ˆ)\n",
        "    return output, attention_weights\n",
        "\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸\n",
        "x = torch.randn(3, 20, 64)\n",
        "out, atw = scaled_dot_product_attention(x, x, x, mask=None)\n",
        "# context vector(ë¬¸ë§¥ ë²¡í„°): ì–´í…ì…˜ì„ í†µê³¼í•œ ìµœì¢… ì¶œë ¥ê°’\n",
        "print(f'attention output shape : {out.shape}')\n",
        "# ì–´í…ì…˜ ê°€ì¤‘ì¹˜: ë‹¨ì–´ê°€ ë¬¸ì¥ ì•ˆì˜ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì— ì–¼ë§ˆë‚˜ ì§‘ì¤‘í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ„. ì–´í…ì…˜ ìŠ¤ì½”ì–´(ìœ ì‚¬ë„)ë¥¼ Softmaxë¥¼ í†µí•´ í™•ë¥ ë¡œ ë‚˜íƒ€ëƒ„\n",
        "print(f'attention weight shape : {atw.shape}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzDG0JWAHp3n",
        "outputId": "0f932092-8361-4fcc-fac3-542113c22841"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention output shape : torch.Size([3, 20, 64])\n",
            "attention weight shape : torch.Size([3, 20, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MultiHead Attention\n",
        "\n",
        "TransformerëŠ” Scaled Dot-Product Attentionì„ ëª¨ë“  ì„ë² ë”© ì°¨ì›ì—ì„œ ë°”ë¡œ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì„ë² ë”© ì°¨ì›ì„ íŠ¹ì • ê°œìˆ˜ë¡œ ë¶„í• í•˜ì—¬ Scaled Dot-Product Attentionì²˜ë¦¬ë¥¼ í•©ë‹ˆë‹¤.  \n",
        "\n",
        "MultiHead Attention  ì„ë² ë”© ì°¨ì›ì„ ì—¬ëŸ¬ê°œì˜ Headë¡œ ë¶„í• í•˜ì—¬ Scaled Dot-Product Attentionì²˜ë¦¬ë¥¼ í•  ë¿ë§Œ ì•„ë‹ˆë¼ ê° Q,K,Vì— ì„ í˜•ë ˆì´ì–´ë¥¼ ì ìš©í•˜ì—¬ í•™ìŠµ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ë¥¼ í• ë‹¹í•´ì£¼ê³  Attention ì¶œë ¥ ì°¨ì›ì—ë„ ì„ í˜•ë ˆì´ì–´ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1YjvM96xK4L3TMWeBfVmDQzTjC_cWZCZs\" width=\"800\"/></center>\n",
        "\n",
        "ì´ëŸ° ë©€í…Œí—¤ë“œ ì–´í…ì…˜ì„ ì ìš©í•¨ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì§•ì„ ì–»ìŠµë‹ˆë‹¤.\n",
        "- HeadëŠ” ì„œë¡œ ë‹¤ë¥¸ (ì„ë² ë”©)ë¶€ë¶„ ê³µê°„ì—ì„œ Q, K, Vë¥¼ í•™ìŠµí•˜ì—¬ ë‹¤ì–‘í•œ ê´€ì (íŒ¨í„´, ì—°ê´€ì„±)ì„ ë™ì‹œì— í¬ì°©\n",
        "- ë³‘ë ¬ë¡œ Attentionì„ ìˆ˜í–‰í•˜ì—¬ ë³´ë‹¤ ë³µì¡í•˜ê³  ì„¸ë°€í•œ ê´€ê³„ë¥¼ í•™ìŠµ\n",
        "- Headë“¤ì˜ Attention ê²°ê³¼ë¥¼ ê²°í•©(Summarize)í•¨ìœ¼ë¡œì¨ ë³´ë‹¤ ì•ˆì •ì ì´ê³  ê· í˜• ì¡íŒ ì •ë³´ë¥¼ ì–»ìŒ\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "semfYDYApxCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, em_dim, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()        # ë¶€ëª¨ í´ë˜ìŠ¤ ì´ˆê¸°í™” í•¨ìˆ˜ í˜¸ì¶œ\n",
        "        self.num_heads = num_heads      # í—¤ë“œ ê°¯ìˆ˜\n",
        "        self.em_dim = em_dim            # ì„ë² ë”© ì°¨ì›\n",
        "\n",
        "        # í—¤ë“œ ë¶„í• ì´ ë¶ˆê°€ëŠ¥ í•˜ë©´ ì—ëŸ¬\n",
        "        # ì„ë² ë”© ì°¨ì›ì´ í—¤ë“œ ê°œìˆ˜ë¡œ ë‚˜ëˆ„ì–´ë–¨ì–´ì§€ëŠ”ì§€ í™•ì¸ (í—¤ë“œë³„ ë™ì¼ ì°¨ì› ë³´ì¥)\n",
        "        assert em_dim % num_heads == 0, \"em_dim must be divisible by num_heads\"\n",
        "\n",
        "        # ë¶„ë¦¬ëœ ì„ë² ë”© ê¸¸ì´\n",
        "        self.depth = em_dim // num_heads\n",
        "\n",
        "        # ì„ í˜•ë ˆì´ì–´\n",
        "        self.wq = nn.Linear(em_dim, em_dim)\n",
        "        self.wk = nn.Linear(em_dim, em_dim)\n",
        "        self.wv = nn.Linear(em_dim, em_dim)\n",
        "\n",
        "        self.dense = nn.Linear(em_dim, em_dim)\n",
        "\n",
        "    # í—¤ë“œ ë¶„í•  í•¨ìˆ˜: ì…ë ¥ ì„ë² ë”©ì„ ì—¬ëŸ¬ ê°œì˜ â€œí—¤ë“œ(head)â€ë¡œ ë‚˜ëˆ„ëŠ” ì—­í• \n",
        "    # ìµœì¢… ì¶œë ¥ shape: (batch_size, num_heads, seq_len, depth)\n",
        "    def split_heads(self, x):\n",
        "        '''\n",
        "            ì˜ˆ)\n",
        "            [------512ì°¨ì›------]\n",
        "            â†“ ë¶„í•  (8ê°œ í—¤ë“œ)\n",
        "            [64][64][64][64][64][64][64][64]\n",
        "        '''\n",
        "        batch_size, seq_len, em_dim = x.size()                      # ì˜ˆ: (32, 20, 512) (batch_size, seq_len, em_dim)\n",
        "        x = x.view(batch_size, seq_len, self.num_heads, self.depth) # ì˜ˆ: (32, 20, 512) â†’ (32, 20, 8, 64) (batch_size, seq_len, num_heads, depth)\n",
        "        x = x.permute(0, 2, 1, 3)                                   # ì˜ˆ: (32, 20, 8, 64) â†’ (32, 8, 20, 64) (batch_size, num_heads, seq_len, depth)\n",
        "        return x\n",
        "\n",
        "    def forward(self, v, k, q, mask=None):\n",
        "        \"\"\"\n",
        "        v, k, q: (batch_size, seq_len, em_dim)\n",
        "        mask   : (batch_size, 1, seq_len, seq_len)\n",
        "        \"\"\"\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        # q,k,v ê°€ì¤‘ì¹˜ í•™ìŠµ: (batch_size, seq_len, em_dim)\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        # q, k, v í—¤ë“œ ë¶„í•  (ê° í—¤ë“œë³„ë¡œ ë‚˜ëˆ”)\n",
        "        # ë‚´ë¶€ ë™ì‘: (batch, seq_len, em_dim) â†’ (batch, seq_len, num_heads, depth) â†’ (batch, num_heads, seq_len, depth)\n",
        "        q = self.split_heads(q)\n",
        "        k = self.split_heads(k)\n",
        "        v = self.split_heads(v)\n",
        "\n",
        "        # ì–´í…ì…˜ ê³„ì‚° ì…ë ¥ shape: (batch_size, num_heads, seq_len, depth)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "\n",
        "        # ì–´í…ì…˜ ê²°ê³¼ë¥¼ ë‹¤ì‹œ (batch_size, seq_len, em_dim)ë¡œ ë˜ëŒë¦¼\n",
        "        scaled_attention = scaled_attention.permute(0, 2, 1, 3).contiguous()  # (batch_size, seq_len, num_heads, depth)\n",
        "        concat_attention = scaled_attention.view(batch_size, -1, self.em_dim) # (batch_size, seq_len, em_dim)\n",
        "\n",
        "        # ì—¬ëŸ¬ í—¤ë“œì˜ ê²°ê³¼ë¥¼ í•˜ë‚˜ë¡œ ê²°í•©í•˜ê³ , í•™ìŠµ ê°€ëŠ¥í•œ ì„ í˜• ë³€í™˜ì„ í†µí•´ ìµœì¢… ì–´í…ì…˜ ì¶œë ¥ì„ ë§Œë“œëŠ” ë‹¨ê³„\n",
        "        output = self.dense(concat_attention)\n",
        "        return output, attention_weights\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸\n",
        "x = torch.randn(2, 10, 64)                      # (batch=2, seq_len=10, em_dim=64)\n",
        "mh = MultiHeadAttention(64, 2)                  # (ì„ë² ë”© ì°¨ì› 64, í—¤ë“œ 2ê°œ)\n",
        "out, atw = mh(x,x,x)\n",
        "print(f'attention output shape : {out.shape}')\n",
        "print(f'attention weight shape : {atw.shape}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HB_0ZIyIx5-",
        "outputId": "8dbf991c-168b-43db-e698-c0412f83b819"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention output shape : torch.Size([2, 10, 64])\n",
            "attention weight shape : torch.Size([2, 2, 10, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MultiHeadAttention ì°¨ì› ë³€í™”\n",
        "\n",
        "| ë‹¨ê³„                             | ì—°ì‚° / í•¨ìˆ˜                                       | í…ì„œ shape                                          | ì„¤ëª…                             |\n",
        "| ------------------------------ | --------------------------------------------- | ------------------------------------------------- | ------------------------------ |\n",
        "| â‘  ì…ë ¥                           | `x = torch.randn(2, 10, 64)`                  | **(batch=2, seq_len=10, em_dim=64)**              | 2ê°œ ë¬¸ì¥, ê° ë¬¸ì¥ ê¸¸ì´ 10, ì„ë² ë”© ì°¨ì› 64   |\n",
        "| â‘¡ ì„ í˜• ë³€í™˜                        | `wq(x), wk(x), wv(x)`                         | (2, 10, 64)                                       | ê° í† í°ì„ Query/Key/Value ê³µê°„ìœ¼ë¡œ íˆ¬ì˜  |\n",
        "| â‘¢ í—¤ë“œ ë¶„í•  (view)                 | `.view(batch, seq_len, num_heads, depth)`     | (2, 10, 2, 32)                                    | 64ì°¨ì›ì„ 2í—¤ë“œë¡œ ë¶„í•  (depth = 32)     |\n",
        "| â‘£ ì¶• ì¬ë°°ì—´ (permute)              | `.permute(0, 2, 1, 3)`                        | **(2, 2, 10, 32)**                                | í—¤ë“œ ì°¨ì›ì„ ì•ìœ¼ë¡œ ì´ë™                  |\n",
        "| â‘¤ Scaled Dot-Product Attention | `scaled_dot_product_attention(q, k, v, mask)` | **ì¶œë ¥:** (2, 2, 10, 32)<br>**ê°€ì¤‘ì¹˜:** (2, 2, 10, 10) | í—¤ë“œë³„ ì–´í…ì…˜ ê³„ì‚° (ê°€ì¤‘ì¹˜: QueryÃ—Key ê´€ê³„) |\n",
        "| â‘¥ ì¶• ì¬ë°°ì—´ (permute back)         | `.permute(0, 2, 1, 3)`                        | (2, 10, 2, 32)                                    | ë‹¤ì‹œ seq_len ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬             |\n",
        "| â‘¦ í—¤ë“œ ê²°í•© (view)                 | `.view(batch, seq_len, em_dim)`               | **(2, 10, 64)**                                   | ì—¬ëŸ¬ í—¤ë“œë¥¼ concatí•˜ì—¬ ì›ë˜ ì„ë² ë”© ì°¨ì› ë³µì›   |\n",
        "| â‘§ ìµœì¢… ì„ í˜• ë³€í™˜                     | `self.dense(concat_attention)`                | **(2, 10, 64)**                                   | ëª¨ë“  í—¤ë“œ ê²°ê³¼ë¥¼ í†µí•©í•´ ìµœì¢… ì¶œë ¥ ìƒì„±         |\n"
      ],
      "metadata": {
        "id": "FO-CN9Jv3xWO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> pytorchì—ëŠ” ë©€í‹°í—¤ë“œì–´í…ì…˜ì„ êµ¬í˜„í•œ nn.MultiheadAttention ë ˆì–´ê°€ ì¡´ì¬í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "ZzktRwklw3bZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ì–´í…ì…˜ ë§ˆìŠ¤í¬\n",
        "\n",
        "TransformerëŠ” í•™ìŠµ íš¨ìœ¨ì„ ìœ„í•´ í¬ê²Œ ë‘ê°€ì§€ì˜ ë§ˆìŠ¤í¬ë¥¼ í™œìš©í•˜ì—¬ ì—°ì‚°ì˜ íš¨ìœ¨ì„ ì–»ìŠµë‹ˆë‹¤.\n",
        "\n",
        "- padding mask\n",
        "    * í† í° ê¸¸ì´(seq_len)ì— ëª» ë¯¸ì³ íŒ¨ë”©ì´ ëœ ê²½ìš° ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´ê°€ ì—°ì‚°ì— ì ìš©ë˜ì§€ ì•Šê²Œ í•˜ê¸° ìœ„í•œ ë§ˆìŠ¤í¬\n",
        "    * ì¸ì½”ë”ì™€ ë””ì½”ë”ì—ì„œ ë‘˜ë‹¤ ì‚¬ìš©\n",
        "\n",
        "- look-ahead mask(ë¯¸ë¦¬ë³´ê¸° ë°©ì§€ ë§ˆìŠ¤í¬)   \n",
        "    * ë””ì½”ë”ì˜ ìê¸°íšŒê·€(attention) ê³¼ì •ì—ì„œ ë¯¸ë˜ í† í°ì„ ì°¸ì¡°í•˜ì§€ ëª»í•˜ë„ë¡ ë§‰ëŠ” ê²ƒ   \n",
        "    * í•™ìŠµ ê³¼ì •ì—ì„œ ë””ì½”ë”ê°€ ë¯¸ë˜ í† í°ì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•œ í•™ìŠµì„ ì§„í–‰ í• ë•Œ ë¼ë²¨ì—ì„œ ë¯¸ë˜ í† í°ì„ ë§ˆìŠ¤í¬ ì²˜ë¦¬í•˜ì—¬ ì§€ì›€    \n",
        "    * ì´ë¥¼ í†µí•´ ë””ì½”ë”ê°€ ë°˜ë³µì ì¸ í•™ìŠµ ì—†ì´ í•œë²ˆì˜ ë³‘ë ¬ ì—°ì‚°ìœ¼ë¡œ ì²˜ë¦¬ë¨   \n",
        "    * Causal Mask ìš©ì–´ë¡œ í‘œí˜„í•˜ê¸°ë„ í•¨"
      ],
      "metadata": {
        "id": "vF922LCRKRrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# íŒ¨ë”©ëœ í† í°(pad)ì´ ìˆëŠ” ë¶€ë¶„ì€ ì–´í…ì…˜ ê³„ì‚°ì—ì„œ ë¬´ì‹œ\n",
        "def create_padding_mask(seq, pad_token=0):\n",
        "  '''\n",
        "    # ì˜ˆ: seq = tensor([[7, 6, 2, 0, 0]])\n",
        "    # pad_token = 0 â†’ 0ì¸ ìœ„ì¹˜ëŠ” íŒ¨ë”©ì„\n",
        "\n",
        "    # (seq != 0) â†’ tensor([[True, True, True, False, False]])\n",
        "    # unsqueeze(1) â†’ (batch_size, 1, seq_len)\n",
        "    # unsqueeze(2) â†’ (batch_size, 1, 1, seq_len)\n",
        "    # True: ì‹¤ì œ ë‹¨ì–´, False: íŒ¨ë”© ìœ„ì¹˜\n",
        "    # ì˜ˆì‹œ ê²°ê³¼: [[[[1, 1, 1, 0, 0]]]]\n",
        "  '''\n",
        "  mask = (seq != pad_token).unsqueeze(1).unsqueeze(2)\n",
        "  # (batch_size, 1, 1, seq_len)\n",
        "  return mask\n",
        "\n",
        "# ë””ì½”ë”ì—ì„œ ë¯¸ë˜ ë‹¨ì–´ë¥¼ ë³´ì§€ ëª»í•˜ê²Œ(look ahead ë°©ì§€): ì‚¼ê°í–‰ë ¬ë¡œ ë§ˆìŠ¤í¬ í–‰ë ¬ êµ¬í˜„\n",
        "def create_look_ahead_mask(size):\n",
        "  '''\n",
        "    ì˜ˆ: size = 5 â†’ ë¬¸ì¥ ê¸¸ì´(seq_len)ê°€ 5ì¸ ê²½ìš°\n",
        "\n",
        "    # torch.tril â†’ í•˜ì‚¼ê°(lower triangle) ë¶€ë¶„ë§Œ Trueë¡œ ìœ ì§€\n",
        "    # [[1,0,0,0,0],\n",
        "    #  [1,1,0,0,0],\n",
        "    #  [1,1,1,0,0],\n",
        "    #  [1,1,1,1,0],\n",
        "    #  [1,1,1,1,1]]\n",
        "  '''\n",
        "  mask = torch.ones(size, size, dtype=torch.bool)\n",
        "  mask = torch.tril(mask)   # í•˜ì‚¼ê° ë¶€ë¶„ë§Œ True\n",
        "  return mask               # (size, size) ì•„ë˜ ì‚¼ê°í˜•ë§Œ True\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸\n",
        "inp = torch.Tensor([[1, 2, 3, 4, 5, 6, 0, 0, 0, 0],\n",
        "                    [3, 5, 6, 2, 6, 5, 8, 0, 0, 0]])\n",
        "\n",
        "pad_mask = create_padding_mask(inp)\n",
        "lh_mask = create_look_ahead_mask(10)\n",
        "print(f'padding mask : \\n{pad_mask}') #(batch_size, 1, seq_len, seq_len)\n",
        "print(f'look ahead mask : \\n{lh_mask}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSytFu8AJlJu",
        "outputId": "c0251883-c981-41f1-9a59-dc555f17681f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "padding mask : \n",
            "tensor([[[[ True,  True,  True,  True,  True,  True, False, False, False, False]]],\n",
            "\n",
            "\n",
            "        [[[ True,  True,  True,  True,  True,  True,  True, False, False, False]]]])\n",
            "look ahead mask : \n",
            "tensor([[ True, False, False, False, False, False, False, False, False, False],\n",
            "        [ True,  True, False, False, False, False, False, False, False, False],\n",
            "        [ True,  True,  True, False, False, False, False, False, False, False],\n",
            "        [ True,  True,  True,  True, False, False, False, False, False, False],\n",
            "        [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
            "        [ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
            "        [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
            "        [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
            "        [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
            "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> nn.MultiheadAttention ì‚¬ìš©ì‹œ ë§ˆìŠ¤í¬ ì²˜ë¦¬ë  ë¶€ë¶„ì´ Trueê°€ ë˜ì–´ì•¼ í•˜ë¯€ë¡œ ë°˜ëŒ€ë¡œ boolì„ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "YR5zzXNExHbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(2, 10, 64)\n",
        "out, atw = mh(x, x, x, mask=lh_mask) # Look-Ahead Mask (ë£©ì–´í—¤ë“œ ë§ˆìŠ¤í¬)\n",
        "print(out.shape)\n",
        "atw[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvGf-fXFKEyw",
        "outputId": "d139302b-8234-43df-91b0-ef51c24ebaf6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 10, 64])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.5358, 0.4642, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.3444, 0.4878, 0.1678, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.1632, 0.1301, 0.3484, 0.3582, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.1786, 0.2081, 0.2301, 0.1695, 0.2137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.2499, 0.1628, 0.0930, 0.1518, 0.1839, 0.1587, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0901, 0.0800, 0.1532, 0.2383, 0.1019, 0.1592, 0.1772, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.1378, 0.2283, 0.1467, 0.1417, 0.0490, 0.1179, 0.1425, 0.0362, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0996, 0.1361, 0.1493, 0.1375, 0.0899, 0.1125, 0.0674, 0.0957, 0.1121,\n",
              "         0.0000],\n",
              "        [0.1113, 0.1092, 0.1110, 0.0944, 0.0726, 0.0971, 0.0640, 0.1111, 0.1333,\n",
              "         0.0960]], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ì¸ì½”ë” ë ˆì´ì–´\n",
        "\n",
        "- Point-wise Feed Forward Network (PFFN)\n",
        "    * ë©€í‹°í—¤ë“œì–´í…ì…˜ ê³„ì¸µë§Œ í™œìš©í•˜ë©´ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ê°€ì¤‘ì¹˜ê°€ ë„ˆë¬´ ì‘ìŒ\n",
        "    * ì„ í˜• ë ˆì´ì–´ì™€ í™œì„±í™” í•¨ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê° í† í°ì˜ ì„ë² ë”©ì„ ë” í’ë¶€í•˜ê²Œ ë³€í™˜í•˜ì—¬ ëª¨ë¸ì˜ í‘œí˜„ë ¥ì„ ë†’ì´ëŠ” ì—­í• \n",
        "\n",
        "- ì¸ì½”ë” ë ˆì´ì–´\n",
        "    * ì…ë ¥ ë¬¸ì¥ì— ëŒ€í•œ íŠ¹ì„±ì„ í•™ìŠµ í•˜ëŠ” ë ˆì´ì–´ë¡œ í•˜ë‚˜ì˜ ë©€í‹°í—¤ë“œì–´í…ì…˜ê³¼ PFFN ë ˆì´ì–´ê°€ ì‚¬ìš©\n",
        "    * ë©€í‹°í—¤ë“œì–´í…ì…˜ì—” í•˜ë‚˜ì˜ ë¬¸ì¥ì´ q,k,vë¡œ ì…ë ¥(ì…€í”„ì–´í…ì…˜)\n",
        "    * ì–´í…ì…˜ ê²°ê³¼ëŠ” ì…ë ¥ ì„ë² ë”©ê³¼ ë”í•˜ì—¬ LayerNormë¥¼ í†µê³¼í•˜ê³  PFFN ë ˆì–´ì— ì…ë ¥\n",
        "    * ì–´í…ì…˜ ê²°ê³¼ì™€ PFFN ê²°ê³¼ë¥¼ ë”í•˜ì—¬ LayerNormì— í†µê³¼í•˜ì—¬ ìµœì¢… ì¶œë ¥\n",
        "\n",
        "> ì–´í…ì…˜ ì¶œë ¥ë¥¼ ì…ë ¥ê³¼ ë”í•˜ëŠ” **Skip Connection**(Residual Connection) ê³¼ì •ì€ ë”¥ëŸ¬ë‹ì—ì„œ ì¸µì´ ê¹Šì–´ì§ˆìˆ˜ë¡ ê¸°ìš¸ê¸°ê°€ ì†Œì‹¤/í­ì£¼í•˜ëŠ” ë¬¸ì œë¥¼ ì™„í•˜í•˜ì—¬ í•™ìŠµ ì•ˆì •ì„±ì„ í–¥ìƒ ì‹œí‚µë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "ryIlZMFNNOPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ê° ë‹¨ì–´ ë²¡í„°ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ë¹„ì„ í˜• í•¨ìˆ˜(GELU)ì— í†µê³¼ì‹œì¼œ, ë‹¨ì–´ë³„ë¡œ ë” í’ë¶€í•œ í‘œí˜„ì„ í•™ìŠµí•˜ê²Œ í•˜ëŠ” ì¸µ\n",
        "def point_wise_feed_forward_network(em_dim, feed_dim):\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(em_dim, feed_dim),    # ì…ë ¥ ì„ë² ë”©(em_dim)ì„ ë” í° ì°¨ì›(feed_dim)ìœ¼ë¡œ í™•ì¥\n",
        "        nn.GELU(),                      # ReLUë³´ë‹¤ ë¶€ë“œëŸ½ê³ , BERT/GPT ë“± ìµœì‹  íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œ í‘œì¤€ìœ¼ë¡œ ì‚¬ìš©\n",
        "        nn.Linear(feed_dim, em_dim)     # ë‹¤ì‹œ ì›ë˜ ì°¨ì›ìœ¼ë¡œ ì¤„ì´ëŠ” ë‹¨ê³„\n",
        "    )"
      ],
      "metadata": {
        "id": "8cqSjGJgL5Tu"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer ì¸ì½”ë” ë¸”ë¡(Encoder Block) í•˜ë‚˜ë¥¼ êµ¬í˜„í•œ ì½”ë“œ\n",
        "class EncoderLayer(nn.Module):\n",
        "  '''\n",
        "  í•œ ê°œì˜ Encoder LayerëŠ” â‘  Multi-Head Attention â†’ â‘¡ Feed Forward Network ë‘ ë¸”ë¡ìœ¼ë¡œ êµ¬ì„±\n",
        "  ê° ë¸”ë¡ ë’¤ì—ëŠ” Residual Connection(ì”ì°¨ ì—°ê²°) + Layer Normalization ì´ ìˆìŠµë‹ˆë‹¤.\n",
        "  '''\n",
        "  def __init__(self, em_dim, num_heads, feed_dim, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "    self.mha = MultiHeadAttention(em_dim, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(em_dim, feed_dim)\n",
        "\n",
        "    # ë‘ ê°œì˜ ì„œë¸Œì¸µ(sub-layer): Multi-Head Attention, Feed Forward Network\n",
        "    self.layernorm1 = nn.LayerNorm(em_dim, eps=1e-6)  # í•™ìŠµì„ ì•ˆì •í™”í•˜ê³ , ì…ë ¥ ë¶„í¬ë¥¼ ì •ê·œí™”\n",
        "    self.layernorm2 = nn.LayerNorm(em_dim, eps=1e-6)\n",
        "    self.dropout1 = nn.Dropout(rate)                  # ê³¼ì í•© ë°©ì§€ìš©, ì¼ë¶€ ë‰´ëŸ°ì„ í™•ë¥ ì ìœ¼ë¡œ ë¹„í™œì„±í™”\n",
        "    self.dropout2 = nn.Dropout(rate)\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "    # Multi-head attention: ë¬¸ë§¥ì„ í•™ìŠµ\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, seq_len, em_dim)\n",
        "    attn_output = self.dropout1(attn_output)\n",
        "    out1 = self.layernorm1(x + attn_output)   # (batch_size, seq_len, em_dim)\n",
        "\n",
        "    # Feed Forward: ë‹¨ì–´ë³„ í‘œí˜„ì„ í™•ì¥\n",
        "    ffn_output = self.ffn(out1)\n",
        "    ffn_output = self.dropout2(ffn_output)\n",
        "    out2 = self.layernorm2(out1 + ffn_output) # (batch_size, seq_len, em_dim)\n",
        "\n",
        "    return out2\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸\n",
        "enc_inp = torch.randn(3, 10, 32)\n",
        "enc_layer = EncoderLayer(32, 4, 64)\n",
        "enc_out = enc_layer(x=enc_inp, mask=None)\n",
        "enc_out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdDovFivNWnd",
        "outputId": "1c5c8a10-189c-4c13-a415-f6631149cf70"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 10, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ë””ì½”ë” ë ˆì´ì–´\n",
        "\n",
        "ë””ì½”ë” ë ˆì´ì–´ëŠ” ì˜ˆì¸¡ ë¬¸ì¥ì„ ë§Œë“¤ê¸° ìœ„í•´ ì´ì „ í† í°ìœ¼ë¡œ ë¯¸ë˜ í† í°ì„ í•™ìŠµ í•˜ëŠ” ë ˆì´ì–´ë¡œ 2ê°œì˜ ë©€í‹°í—¤ë“œì–´í…ì…˜ê³¼ í•˜ë‚˜ì˜ PFFNì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "    * ì²«ë²ˆì§¸ ë©€í‹°í—¤ë“œì–´í…ì…˜: ë””ì½”ë” ì…ë ¥ìœ¼ë¡œ ì…€í”„ì–´í…ì…˜ ì§„í–‰\n",
        "    * ì²«ë²ˆì§¸ LayerNorm: ì–´í…ì…˜ ê²°ê³¼ì™€ ë””ì½”ë” ì…ë ¥ì„ ë”í•´ LayerNorm ì§„í–‰\n",
        "    * ë‘ë²ˆì§¸ ë©€í‹°í—¤ë“œì–´í…ì…˜: ì¸ì½”ë”ì˜ ì¶œë ¥ì´ V,Kë¡œ ì…ë ¥ë˜ê³  ì²«ë²ˆì§¸ LayerNorm ì¶œë ¥ì´ Që¡œ ì…ë ¥ë˜ì–´ ì–´í…ì…˜ ì§„í–‰\n",
        "    * ë‘ë²ˆì§¸ LayerNorm: ì²«ë²ˆì§¸ LayerNorm ì¶œë ¥ê³¼ ë‘ë²ˆì§¸ ì–´í…ì…˜ ê²°ê³¼ë¥¼ ë”í•´ LayerNorm ì§„í–‰\n",
        "    * PFFN: ë‘ë²ˆì§¸ LayerNorm ê²°ê³¼ë¥¼ ì…ë ¥í•˜ì—¬ ì¶œë ¥ ê²°ê³¼ë¥¼ ë”í•´ LayerNorm ì§„í–‰"
      ],
      "metadata": {
        "id": "pNYe56bCykie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer ë””ì½”ë” ë¸”ë¡(Decoder Block) í•˜ë‚˜ë¥¼ êµ¬í˜„í•œ ì½”ë“œ: ì¸ì½”ë” ì¶œë ¥ì„ ë°›ì•„ì„œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ìƒì„±í•  ì¤€ë¹„ë¥¼ í•˜ëŠ” í•µì‹¬ ë¸”ë¡\n",
        "class DecoderLayer(nn.Module):\n",
        "  '''\n",
        "    3ê°œì˜ ì„œë¸Œì¸µ(sub-layer)\n",
        "      1) Masked Multi-Head Self-Attention: ë””ì½”ë” ë‚´ë¶€ ë‹¨ì–´ë“¤ë¼ë¦¬ ì–´í…ì…˜ (ë¯¸ë˜ ë‹¨ì–´ ê°€ë¦¬ê¸°)\n",
        "      2) Encoderâ€“Decoder Attention: ì¸ì½”ë”ì˜ ì¶œë ¥ê³¼ ì–´í…ì…˜ (ì…ë ¥ ë¬¸ì¥ ì°¸ê³ )\n",
        "      3) Feed Forward Network (FFN): ê° ë‹¨ì–´ ë²¡í„° ë…ë¦½ì ìœ¼ë¡œ ë¹„ì„ í˜• ë³€í™˜\n",
        "  '''\n",
        "  def __init__(self, em_dim, num_heads, feed_dim, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "    self.mha1 = MultiHeadAttention(em_dim, num_heads)\n",
        "    self.mha2 = MultiHeadAttention(em_dim, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(em_dim, feed_dim)\n",
        "\n",
        "    self.layernorm1 = nn.LayerNorm(em_dim, eps=1e-6)\n",
        "    self.layernorm2 = nn.LayerNorm(em_dim, eps=1e-6)\n",
        "    self.layernorm3 = nn.LayerNorm(em_dim, eps=1e-6)\n",
        "\n",
        "    self.dropout1 = nn.Dropout(rate)\n",
        "    self.dropout2 = nn.Dropout(rate)\n",
        "    self.dropout3 = nn.Dropout(rate)\n",
        "\n",
        "  def forward(self, x, enc_output, look_ahead_mask=None, padding_mask=None):\n",
        "    # Masked Multi-Head Self-Attention: ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•  ë•Œ, ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì€ ë¯¸ë˜ ë‹¨ì–´ë¥¼ ë³´ì§€ ëª»í•˜ê²Œ í•˜ë©´ì„œ ë¬¸ë§¥(ì´ì „ ë‹¨ì–´ë“¤)ë§Œ ì°¸ê³ í•˜ë„ë¡ ë§Œë“§\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)    # look_ahead_mask: ë¯¸ë˜ ë‹¨ì–´ë¥¼ ë³´ì§€ ì•Šë„ë¡ ë§‰ëŠ” ë§ˆìŠ¤í¬\n",
        "    attn1 = self.dropout1(attn1)\n",
        "    out1 = self.layernorm1(x + attn1)\n",
        "\n",
        "    # ì¸ì½”ë” ì¶œë ¥ê³¼ì˜ ì–´í…ì…˜: ì…ë ¥ ë¬¸ì¥(ì¸ì½”ë” ì¶œë ¥) ì˜ ì •ë³´ì™€ í˜„ì¬ê¹Œì§€ ìƒì„±ëœ ë””ì½”ë” ë¬¸ì¥ì„ ì—°ê²°\n",
        "    attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
        "    attn2 = self.dropout2(attn2)\n",
        "    out2 = self.layernorm2(out1 + attn2)\n",
        "\n",
        "    # Feed Forward: ê° ë‹¨ì–´ ë²¡í„°ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ë³€í™˜í•´ ë” í’ë¶€í•œ í‘œí˜„ì„ í•™ìŠµí•˜ëŠ” ì—­í• (GELU ì‹œ)\n",
        "    ffn_output = self.ffn(out2)\n",
        "    ffn_output = self.dropout3(ffn_output)\n",
        "    out3 = self.layernorm3(out2 + ffn_output)\n",
        "\n",
        "    return out3, attn_weights_block1, attn_weights_block2\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸\n",
        "dec_inp = torch.randn(3, 10, 32)\n",
        "dec_layer = DecoderLayer(32, 4, 64)\n",
        "dec_out, w1, w2 = dec_layer(dec_inp, enc_out, None, None)\n",
        "print(w2.shape)"
      ],
      "metadata": {
        "id": "L02nB6xzNZaR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e37d0f0-3041-48f7-87e1-810e0d89fc8e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 4, 10, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### í¬ì§€ì…˜ ì¸ì½”ë”©\n",
        "ë©€í‹°í—¤ë“œ ì–´í…ì…˜ì€ ì…ë ¥ ìˆœì„œë¥¼ ê³ ë ¤í•˜ì§€ ì•ŠëŠ”(Self-Attention) êµ¬ì¡°ì´ë¯€ë¡œ, ë‹¨ì–´ì˜ ìˆœì„œ ì •ë³´ ì¶”ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n",
        "ì´ëŸ¬í•œ ìˆœì„œì •ë³´ë¥¼ ì¶”ê°€í•˜ê¸° ìœ„í•´ í† í° ì„ë² ë”© ê²°ê³¼ì— ìˆœì„œì •ë³´ë¥¼ ë”í•´ì¤ë‹ˆë‹¤.\n",
        "\n",
        "í¬ì§€ì…˜ ì¸ì½”ë”©ì€ ê° í¬ì§€ì…˜ì˜ ê°ë„ë¥¼ êµ¬í•œë’¤ sin, cos í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ ìƒëŒ€ì ì¸ ìœ„ì¹˜ ì •ë³´ ë§Œë“¤ì–´ ëƒ…ë‹ˆë‹¤. ì´ëŠ” ë³‘ë ¬ì²˜ë¦¬ë¥¼ ì§„í–‰í•˜ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ì— ë§¤ìš° íš¨ìœ¨ì ì¸ ìˆœì„œ ì •ë³´ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "\\begin{aligned}\n",
        "\\text{PE}(pos,\\,2i) &= \\sin\\!\\Bigl(\\tfrac{pos}{10000^{\\,\\frac{2i}{d_{\\text{model}}}}}\\Bigr),\\\\[8pt]\n",
        "\\text{PE}(pos,\\,2i+1) &= \\cos\\!\\Bigl(\\tfrac{pos}{10000^{\\,\\frac{2i}{d_{\\text{model}}}}}\\Bigr).\n",
        "\\end{aligned}\n",
        "\n",
        "- ê° í† í°ì˜ ìœ„ì¹˜(pos) ì— ëŒ€í•´, íŠ¹ì • ê°ë„(ì£¼íŒŒìˆ˜)ë¡œ ë³€í™˜í•œ ë’¤, sin(ì§ìˆ˜)ê³¼ cos(í™€ìˆ˜) í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ ì„ë² ë”© ì°¨ì›ë³„ë¡œ ë‹¤ë¥¸ ì£¼ê¸°ë¥¼ ê°–ëŠ” ì‚¬ì¸íŒŒ ìƒì„±\n",
        "- ì„œë¡œ ë‹¤ë¥¸ ìœ„ì¹˜(pos) ê°„ì˜ ìƒëŒ€ì  ê±°ë¦¬ë¥¼ í•¨ìˆ˜ì˜ ìœ„ìƒ ì°¨ì´ë¥¼ í†µí•´ ëª¨ë¸ì´ í•™ìŠµ\n"
      ],
      "metadata": {
        "id": "t0nLkGCYQBMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformerì˜ â€œPositional Encodingâ€ ì„ ì‹œê°í™”\n",
        "# ë¬¸ì¥ ë‚´ ë‹¨ì–´ ìœ„ì¹˜(position)ì— ë”°ë¼ ì‚¬ì¸(sin)ê³¼ ì½”ì‚¬ì¸(cos) ì£¼ê¸°ë¡œ ë³€í•˜ëŠ” ê°’ì„ ë§Œë“¤ì–´, ë‹¨ì–´ì˜ ìœ„ì¹˜ì •ë³´ë¥¼ ë²¡í„°ë¡œ í‘œí˜„í•˜ê³  ì‹œê°í™”\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ìœ„ì¹˜ì— ë”°ë¥¸ ê°ë„ ì¶”ì¶œ\n",
        "def get_angles(pos, i, em_dim):\n",
        "    # í•˜ë‚˜ì˜ ìœ„ì¹˜ì—ì„œ ì„ë² ë”©ì˜ ê¸¸ì´ ë§Œí¼ ê°ë„ê°€ ì¶”ì¶œë¨\n",
        "    # (ìœ„ì¹˜ ê¸¸ì´, ì„ë² ë”© ê¸¸ì´)\n",
        "    return pos / (10000 ** ((2 * (i // 2)) / em_dim))\n",
        "\n",
        "# ê°ë„ë¡œ ë¶€í„° ì‚¬ì¸íŒŒ ìƒì„±\n",
        "def positional_encoding(position, em_dim):\n",
        "    angle_rads = get_angles(\n",
        "        np.arange(position)[:, np.newaxis],\n",
        "        np.arange(em_dim)[np.newaxis, :],\n",
        "        em_dim\n",
        "    )\n",
        "\n",
        "    # ì§ìˆ˜ ì¸ë±ìŠ¤ì—ëŠ” sin, í™€ìˆ˜ ì¸ë±ìŠ¤ì—ëŠ” cos\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]  # ë°°ì¹˜ ì¶• ì¶”ê°€ (1, position, em_dim)\n",
        "    return torch.tensor(pos_encoding, dtype=torch.float32)\n",
        "\n",
        "position = 50\n",
        "em_dim = 16\n",
        "\n",
        "pos_encoding = positional_encoding(position, em_dim)\n",
        "print(pos_encoding.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YklmZIvRQC4I",
        "outputId": "fa6d1aac-e892-4c73-f10c-722967ee475b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 50, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ìµœì¢… ì¸ì½”ë”ì™€ ë””ì½”ë”\n",
        "\n",
        "ì§€ê¸ˆê¹Œì§€ êµ¬í˜„í•œ ì¸ì½”ë” ë ˆì´ì–´ì™€ ë””ì½”ë” ë ˆì´ì–´, ì„ë² ë”© ë ˆì´ì–´, í¬ì§€ì…˜ ì¸ì½”ë”©ì„ ê²°í•©í•˜ì—¬ Transformer ì•„í‚¤í…ì³ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
        "\n",
        "<center><img src=\"https://arxiv.org/html/1706.03762v7/extracted/1706.03762v7/Figures/ModalNet-21.png\" width=\"300\"/></center>"
      ],
      "metadata": {
        "id": "M5ehkEZl3bkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  '''\n",
        "    1) Embedding: ë‹¨ì–´ ID â†’ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜ í›„ ìŠ¤ì¼€ì¼ë§\n",
        "    2) Positional Encoding: ìœ„ì¹˜ ì •ë³´(sin/cos)ë¥¼ ì„ë² ë”©ì— ë”í•¨\n",
        "    3) Dropout: ì…ë ¥ ì„ë² ë”©+í¬ì§€ì…˜ì— ë“œë¡­ì•„ì›ƒ ì ìš©\n",
        "    4) Stacked EncoderLayers (ê° ì¸µë§ˆë‹¤):\n",
        "      - Multi-Head Self-Attention\n",
        "      - Dropout + Residual + LayerNorm\n",
        "      - Feed Forward Network (FFN)\n",
        "      - Dropout + Residual + LayerNorm\n",
        "  '''\n",
        "\n",
        "  def __init__(self, num_layers, em_dim, num_heads, feed_dim,\n",
        "                input_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.em_dim = em_dim\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    # ì„ë² ë”© ë ˆì´ì–´\n",
        "    self.embedding = nn.Embedding(input_vocab_size, em_dim)\n",
        "    # í¬ì§€ì…˜ ì¸ì½”ë”©\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, em_dim)\n",
        "\n",
        "    # ì¸ì½”ë” ë ˆì´ì–´\n",
        "    self.enc_layers = nn.ModuleList([\n",
        "        EncoderLayer(em_dim, num_heads, feed_dim, rate)\n",
        "        for _ in range(num_layers)\n",
        "    ])\n",
        "    self.dropout = nn.Dropout(rate)\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "    seq_len = x.size(1)\n",
        "\n",
        "    # ì„ë² ë”© í›„ ìŠ¤ì¼€ì¼ë§\n",
        "    x = self.embedding(x)  # (batch_size, seq_len, em_dim)\n",
        "    x = x * math.sqrt(self.em_dim)\n",
        "\n",
        "    # í¬ì§€ì…”ë„ ì¸ì½”ë”©\n",
        "    pe = self.pos_encoding[:, :seq_len, :].to(x.device)\n",
        "\n",
        "    # ì„ë² ë”©ê³¼ í¬ì§€ì…˜ ê²°í•©\n",
        "    x = x + pe\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    # ì¸ì½”ë” ë ˆì´ì–´ì¸µ í†µê³¼\n",
        "    for i in range(self.num_layers):\n",
        "        x = self.enc_layers[i](x, mask)\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "ix9QCJJTPjvs"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  '''\n",
        "    1) Embedding + ìŠ¤ì¼€ì¼ë§: ë””ì½”ë” ì…ë ¥ í† í°ì„ ë²¡í„°ë¡œ ë³€í™˜í•˜ê³  í¬ê¸°ë¥¼ ë³´ì •\n",
        "    2) Positional Encoding: ìœ„ì¹˜ ì •ë³´ë¥¼ sin/cos íŒ¨í„´ìœ¼ë¡œ ì„ë² ë”©ì— ë”í•¨\n",
        "    3) Dropout: ì„ë² ë”©+í¬ì§€ì…˜ì— ë“œë¡­ì•„ì›ƒ ì ìš© (ê³¼ì í•© ë°©ì§€)\n",
        "    4) Stacked DecoderLayers (ê° ì¸µë§ˆë‹¤)\n",
        "      - Masked Multi-Head Self-Attention: ë¯¸ë˜ í† í°ì„ ê°€ë¦° ìê¸° ì–´í…ì…˜\n",
        "      - Encoderâ€“Decoder Attention: ì¸ì½”ë” ì¶œë ¥ê³¼ì˜ ê´€ê³„ í•™ìŠµ\n",
        "      - Feed Forward Network (FFN): ê° ë‹¨ì–´ ë²¡í„° ë¹„ì„ í˜• ë³€í™˜\n",
        "      - ê° ë¸”ë¡ë§ˆë‹¤ Dropout + Residual + LayerNormìœ¼ë¡œ ì•ˆì •í™”\n",
        "      - Attention Weights ì €ì¥: block1(ìê¸°), block2(ì¸ì½”ë”-ë””ì½”ë”) ê¸°ë¡\n",
        "  '''\n",
        "  def __init__(self, num_layers, em_dim, num_heads, feed_dim,\n",
        "                target_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.em_dim = em_dim\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = nn.Embedding(target_vocab_size, em_dim)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, em_dim)\n",
        "\n",
        "    self.dec_layers = nn.ModuleList([\n",
        "        DecoderLayer(em_dim, num_heads, feed_dim, rate)\n",
        "        for _ in range(num_layers)\n",
        "    ])\n",
        "    self.dropout = nn.Dropout(rate)\n",
        "\n",
        "  def forward(self, x, enc_output, look_ahead_mask=None, padding_mask=None):\n",
        "    seq_len = x.size(1)\n",
        "    attention_weights = {}\n",
        "\n",
        "    # ë””ì½”ë” ì…ë ¥ì— ëŒ€í•œ ì„ë² ë”© ë° í¬ì§€ì…˜ ì¸ì½”ë”©\n",
        "    x = self.embedding(x)\n",
        "    x = x * math.sqrt(self.em_dim)\n",
        "    pe = self.pos_encoding[:, :seq_len, :].to(x.device)\n",
        "    x = x + pe\n",
        "\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    # ì¸ì½”ë” ì¶œë ¥ê³¼ í•¨ê»˜ ë””ì½”ë” ë ˆì´ì–´ì— ì…ë ¥\n",
        "    for i in range(self.num_layers):\n",
        "        x, block1, block2 = self.dec_layers[i](x, enc_output,\n",
        "                                                look_ahead_mask,\n",
        "                                                padding_mask)\n",
        "        # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ê¸°ë¡(ì–´í…ì…˜ ê°€ì¤‘ì¹˜ í™•ì¸ ìš©ë„)\n",
        "        attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "        attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "    return x, attention_weights\n"
      ],
      "metadata": {
        "id": "i_LTg2BMQX6r"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "eco_inp = torch.Tensor([[1, 2, 3, 4, 5, 6, 0, 0, 0, 0]]).long()\n",
        "dco_inp = torch.Tensor([[1, 2, 3, 4, 5, 6, 7, 8, 0, 0]]).long()\n",
        "pad_mask = create_padding_mask(eco_inp)\n",
        "lh_mask = create_look_ahead_mask(10)\n",
        "\n",
        "ecoder = Encoder(2, 32, 4, 64, 10, 100)\n",
        "eco_out = ecoder(eco_inp, pad_mask)\n",
        "print(f'ecoder out:{eco_out.shape}')\n",
        "decoder = Decoder(2, 32, 4, 64, 10, 100)\n",
        "dec_out, attn = decoder(dco_inp, eco_out, lh_mask, pad_mask)\n",
        "print(f'decoder out:{dec_out.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irIDS_B4QZ2g",
        "outputId": "466c5f86-b059-4aeb-a542-1006bb739a7f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ecoder out:torch.Size([1, 10, 32])\n",
            "decoder out:torch.Size([1, 10, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ë””ì½”ë” 2ë²ˆì§¸ ë¸”ëŸ­ì˜ 0ë²ˆ ë°°ì¹˜ ì–´í…ì…˜ ê°€ì¤‘ì¹˜\n",
        "print('1ë²ˆ ì–´í…ì…˜')\n",
        "print(attn['decoder_layer2_block1'][0])\n",
        "print('2ë²ˆ ì–´í…ì…˜')\n",
        "print(attn['decoder_layer2_block2'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1Zha8Z2Rpzj",
        "outputId": "00d3e59b-1d16-4c6f-a247-8e5ea9ec3f2a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1ë²ˆ ì–´í…ì…˜\n",
            "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.6401, 0.3599, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.2870, 0.3441, 0.3689, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.2640, 0.3489, 0.1772, 0.2099, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1802, 0.1314, 0.1860, 0.3081, 0.1944, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1730, 0.1053, 0.1883, 0.1218, 0.1608, 0.2507, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.2068, 0.1188, 0.1116, 0.1488, 0.1409, 0.1288, 0.1442, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1277, 0.2903, 0.0929, 0.0414, 0.0988, 0.1033, 0.1568, 0.0889,\n",
            "          0.0000, 0.0000],\n",
            "         [0.0941, 0.0840, 0.1076, 0.0463, 0.1095, 0.2154, 0.1686, 0.0888,\n",
            "          0.0856, 0.0000],\n",
            "         [0.0768, 0.1275, 0.0969, 0.0475, 0.1020, 0.1362, 0.1433, 0.0659,\n",
            "          0.1086, 0.0952]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.4998, 0.5002, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.2720, 0.3641, 0.3639, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.2783, 0.1859, 0.2343, 0.3016, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1190, 0.1021, 0.2381, 0.2835, 0.2573, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1333, 0.1297, 0.2327, 0.2040, 0.1746, 0.1256, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1420, 0.0854, 0.1435, 0.1629, 0.1906, 0.1130, 0.1627, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1717, 0.1028, 0.1127, 0.1381, 0.0917, 0.1214, 0.1354, 0.1262,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1315, 0.1351, 0.1408, 0.1118, 0.0980, 0.1058, 0.1472, 0.0868,\n",
            "          0.0430, 0.0000],\n",
            "         [0.1232, 0.0944, 0.1219, 0.1251, 0.1230, 0.1054, 0.1404, 0.0757,\n",
            "          0.0465, 0.0444]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.5770, 0.4230, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.2797, 0.4140, 0.3062, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.4582, 0.1905, 0.2097, 0.1415, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.0944, 0.2400, 0.1907, 0.1540, 0.3210, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.0953, 0.2141, 0.1517, 0.1589, 0.2340, 0.1461, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1723, 0.1448, 0.1334, 0.0835, 0.1621, 0.1646, 0.1394, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1497, 0.1256, 0.1124, 0.1153, 0.0947, 0.1040, 0.1662, 0.1323,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1316, 0.0722, 0.1013, 0.1831, 0.0699, 0.0659, 0.1109, 0.1843,\n",
            "          0.0808, 0.0000],\n",
            "         [0.1162, 0.0782, 0.1085, 0.1554, 0.0719, 0.0661, 0.1048, 0.1458,\n",
            "          0.0860, 0.0670]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.5399, 0.4601, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.4036, 0.3399, 0.2565, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.2832, 0.2545, 0.1895, 0.2727, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.2189, 0.1550, 0.1505, 0.1896, 0.2860, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1313, 0.1462, 0.1485, 0.1805, 0.2071, 0.1863, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1394, 0.1480, 0.1396, 0.2272, 0.1107, 0.1044, 0.1306, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1123, 0.1104, 0.0889, 0.1324, 0.1049, 0.1322, 0.1459, 0.1731,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1453, 0.0828, 0.0701, 0.1221, 0.1044, 0.0970, 0.0965, 0.1202,\n",
            "          0.1616, 0.0000],\n",
            "         [0.1180, 0.0994, 0.0705, 0.1148, 0.0914, 0.0702, 0.0636, 0.1021,\n",
            "          0.1482, 0.1220]]], grad_fn=<SelectBackward0>)\n",
            "2ë²ˆ ì–´í…ì…˜\n",
            "tensor([[[0.2638, 0.1522, 0.1147, 0.0797, 0.2400, 0.1495, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1062, 0.1219, 0.2173, 0.1614, 0.1552, 0.2381, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.0874, 0.1200, 0.2025, 0.2308, 0.1948, 0.1645, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1070, 0.1370, 0.1812, 0.2232, 0.2279, 0.1237, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1246, 0.1669, 0.1770, 0.1547, 0.2545, 0.1224, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.0982, 0.1166, 0.1651, 0.2275, 0.2191, 0.1735, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1437, 0.1506, 0.1843, 0.1788, 0.1530, 0.1897, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1371, 0.0797, 0.1575, 0.1610, 0.1708, 0.2937, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.0982, 0.1229, 0.1628, 0.2097, 0.2568, 0.1497, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.0897, 0.1120, 0.1903, 0.2042, 0.2389, 0.1648, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000]],\n",
            "\n",
            "        [[0.1793, 0.0621, 0.1610, 0.2354, 0.1928, 0.1695, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1319, 0.1102, 0.1616, 0.2254, 0.1980, 0.1729, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1134, 0.1109, 0.1922, 0.1757, 0.2329, 0.1749, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1325, 0.1963, 0.1772, 0.1782, 0.1599, 0.1559, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1359, 0.1452, 0.1351, 0.2229, 0.2206, 0.1403, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1395, 0.0566, 0.0895, 0.3009, 0.2290, 0.1846, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1827, 0.1070, 0.1279, 0.1914, 0.1833, 0.2077, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1578, 0.0775, 0.1581, 0.2404, 0.1507, 0.2155, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1485, 0.0806, 0.1576, 0.2302, 0.2042, 0.1790, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1481, 0.0758, 0.1637, 0.2408, 0.1813, 0.1903, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000]],\n",
            "\n",
            "        [[0.2507, 0.1568, 0.0741, 0.1359, 0.2548, 0.1276, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1056, 0.2467, 0.1885, 0.1346, 0.1943, 0.1303, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1088, 0.1971, 0.2287, 0.1416, 0.1486, 0.1752, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1296, 0.1294, 0.2371, 0.1731, 0.0822, 0.2485, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.0825, 0.2525, 0.2201, 0.1326, 0.1622, 0.1502, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1047, 0.2555, 0.1236, 0.1182, 0.3005, 0.0976, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1305, 0.1727, 0.1336, 0.1949, 0.1773, 0.1909, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.2761, 0.1848, 0.1410, 0.1186, 0.1694, 0.1101, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1359, 0.2362, 0.1500, 0.0868, 0.3020, 0.0891, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1633, 0.2083, 0.1684, 0.1007, 0.2550, 0.1043, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000]],\n",
            "\n",
            "        [[0.1117, 0.1977, 0.1255, 0.1569, 0.1934, 0.2147, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1329, 0.1647, 0.1893, 0.1707, 0.1782, 0.1642, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1946, 0.1673, 0.1516, 0.1576, 0.1417, 0.1871, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1678, 0.1746, 0.1856, 0.1350, 0.1780, 0.1589, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1377, 0.1549, 0.0962, 0.1093, 0.2855, 0.2164, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1408, 0.1684, 0.1090, 0.1822, 0.1674, 0.2323, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1349, 0.2201, 0.1952, 0.1253, 0.2149, 0.1096, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1366, 0.1354, 0.0980, 0.1870, 0.1499, 0.2932, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1554, 0.1282, 0.0951, 0.2506, 0.1010, 0.2696, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000],\n",
            "         [0.1312, 0.1378, 0.1148, 0.2397, 0.1138, 0.2628, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000]]], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer ëª¨ë¸ë§\n",
        "\n",
        "ì¸ì½”ë”ì™€ ë””ì½”ë”ë¥¼ ê²°í•©í•˜ê³  ìµœì¢… ë¶„ë¥˜ê¸°ë¥¼ ë”í•œ Transformer êµ¬ì¡°ë¥¼ ë§Œë“­ë‹ˆë‹¤. ì´ë•Œ ì¸ì½”ë”ì™€ ë””ì½”ë”ì— ë„£ì–´ì¤„ ë§ˆìŠ¤í¬ë¥¼ `forward` ë‹¨ê³„ì—ì„œ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "\n",
        "ì™„ì„±ëœ Transformerëª¨ë¸ì˜ `forward`ëŠ” í•™ìŠµì„ ìœ„í•œ ìˆœì „íŒŒë¡œ íƒ€ê²Ÿì´ ê°™ì´ ì…ë ¥ ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ë”°ë¼ì„œ í•™ìŠµëœ Transformerëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ì‘ì—…ì€ `forward` í•¨ìˆ˜ë¥¼ í™œìš©í•˜ê¸°ì— ì ì ˆ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
        "í•´ë‹¹ ì˜ˆì‹œì—ì„ ëŠ í•™ìŠµëœ Transformerë¥¼ í™œìš©í•´ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•œ `generate` í•¨ìˆ˜ë¥¼ ë”°ë¡œ êµ¬í˜„ í•©ë‹ˆë‹¤.\n",
        "\n",
        "**generate í•¨ìˆ˜**\n",
        "- ì¸ì½”ë” ì…ë ¥ë  í† í°ì…‹ê³¼ ë””ì½”ë”ì— ì…ë ¥ë  ì‹œì‘ í† í° í•˜ë‚˜ë§Œ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ìê¸°íšŒê·€(Autoregressive)ë°©ì‹ìœ¼ë¡œ í† í°ì„ ìƒì„±\n",
        "- ìµœëŒ€ í† í° ê¸¸ì´ë¥¼ ì„¤ì •í•˜ì—¬ ë””ì½”ë”ì˜ í† í° ì˜ˆì¸¡ì„ ë°˜ë³µí•˜ê³  ë¬¸ì¥ ë í† í°ì´ ë‚˜ì˜¤ëŠ” ê²½ìš° ë°˜ë³µì„ ì¤‘ë‹¨\n",
        "- í† í° idë¥¼ ì‹¤ì œ ë‹¨ì–´ í† í°ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì¶œë ¥\n",
        "\n"
      ],
      "metadata": {
        "id": "CV0ADvav4P_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, num_layers, em_dim, num_heads, feed_dim,\n",
        "                input_vocab_size, target_vocab_size,\n",
        "                pe_input, pe_target, rate=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder(num_layers, em_dim, num_heads, feed_dim,\n",
        "                            input_vocab_size, pe_input, rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers, em_dim, num_heads, feed_dim,\n",
        "                            target_vocab_size, pe_target, rate)\n",
        "\n",
        "    self.final_layer = nn.Linear(em_dim, target_vocab_size)\n",
        "\n",
        "  def create_masks(self, inp, tar, pad_token=0):\n",
        "    # ì¸ì½”ë” íŒ¨ë”© ë§ˆìŠ¤í¬\n",
        "    enc_padding_mask = create_padding_mask(inp, pad_token)\n",
        "\n",
        "    # ë””ì½”ë” íŒ¨ë”© ë§ˆìŠ¤í¬\n",
        "    dec_padding_mask = create_padding_mask(inp, pad_token)\n",
        "\n",
        "    # look_ahead_mask\n",
        "    seq_len = tar.size(1)\n",
        "    look_ahead = create_look_ahead_mask(seq_len).to(tar.device)  # (seq_len, seq_len)\n",
        "    look_ahead = look_ahead.unsqueeze(0).unsqueeze(0)  # (1,1,seq_len,seq_len)\n",
        "\n",
        "    # íƒ€ê²Ÿ(ë””ì½”ë” ì…ë ¥)ì—ë„ íŒ¨ë”©ì´ ìˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ\n",
        "    dec_target_padding_mask = create_padding_mask(tar, pad_token)  # (batch_size, 1, 1, seq_len)\n",
        "    combined_mask = look_ahead & dec_target_padding_mask  # ë‘˜ ë‹¤ Trueì—¬ì•¼ True\n",
        "\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
        "\n",
        "  def forward(self, inp, tar, pad_token=0):\n",
        "    \"\"\"\n",
        "    inp, tar: (batch_size, seq_len)\n",
        "    \"\"\"\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = self.create_masks(inp, tar, pad_token)\n",
        "\n",
        "    enc_output = self.encoder(inp, enc_padding_mask)  # (batch_size, inp_seq_len, em_dim)\n",
        "    dec_output, attention_weights = self.decoder(\n",
        "        tar, enc_output, look_ahead_mask=combined_mask, padding_mask=dec_padding_mask\n",
        "    )\n",
        "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "    return final_output, attention_weights\n",
        "\n",
        "  # í…ìŠ¤íŠ¸ ìƒì„± í•¨ìˆ˜(Autoregressive ë°©ì‹)\n",
        "  @torch.no_grad()\n",
        "  def generate(self, enc_input, start_tokens, end_token=None, pad_token=0, max_new_tokens=50, device='cpu'):\n",
        "    '''\n",
        "    enc_input : ì¸ì½”ë”ì— ì…ë ¥ë  í† í°ì…‹(ì§ˆë¬¸)\n",
        "    start_tokens : ë””ì½”ë”ì˜ ì‹œì‘ í† í°\n",
        "    end_token : ë””ì½”ë”ë¥¼ ë©ˆì¶”ê¸° ìœ„í•œ ëí† í°\n",
        "    pad_token : ë§ˆìŠ¤í¬ ìƒì„±ì„ ìœ„í•œ íŒ¨ë”© í† í°\n",
        "    max_new_tokens : ìƒì„±ë  í† í°ì˜ ìµœëŒ€ ê°œìˆ˜\n",
        "    '''\n",
        "\n",
        "    # ====== ì¶”ê°€\n",
        "    enc_input = enc_input.to(device)\n",
        "    # ====== ì¶”ê°€\n",
        "\n",
        "    enc_padding_mask = create_padding_mask(enc_input, pad_token)\n",
        "    enc_output = self.encoder(enc_input, enc_padding_mask.to(device))\n",
        "\n",
        "    # ë™ì ìœ¼ë¡œ ì…ë ¥ í† í°ì„ ë§Œë“¤ì–´ëƒ„\n",
        "    generated_tokens = [start_tokens]\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        # í˜„ì¬ê¹Œì§€ ìƒì„±ëœ í† í°\n",
        "        cur_tokens = torch.tensor(generated_tokens, dtype=torch.long, device=device)\n",
        "        cur_tokens = cur_tokens.unsqueeze(0)  # (1, current_length)\n",
        "\n",
        "        # ë§ˆìŠ¤í¬ ìƒì„±\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = self.create_masks(enc_input, cur_tokens, pad_token)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            dec_output, _ = self.decoder(\n",
        "                cur_tokens, enc_output,\n",
        "                look_ahead_mask=combined_mask,\n",
        "                padding_mask=dec_padding_mask\n",
        "            )\n",
        "            # ë§ˆì§€ë§‰ í† í° ìœ„ì¹˜ì˜ ì¶œë ¥ë§Œ ì¶”ì¶œ\n",
        "            logits = self.final_layer(dec_output)  # (1, cur_len, vocab_size)\n",
        "            next_token_logits = logits[:, -1, :]    # (1, vocab_size)\n",
        "\n",
        "            # ê·¸ë¦¬ë””: í™•ë¥ ì´ ê°€ì¥ í° í† í° 1ê°œ\n",
        "            next_token = next_token_logits.argmax(dim=-1).item()\n",
        "\n",
        "        generated_tokens.append(next_token)\n",
        "\n",
        "        # ì¢…ë£Œ í† í°ì„ ìƒì„±í•˜ë©´ ì¤‘ë‹¨\n",
        "        if end_token is not None and next_token == end_token:\n",
        "            break\n",
        "\n",
        "    return generated_tokens\n"
      ],
      "metadata": {
        "id": "5ifuRlPXRWxe"
      },
      "execution_count": 41,
      "outputs": []
    }
  ]
}